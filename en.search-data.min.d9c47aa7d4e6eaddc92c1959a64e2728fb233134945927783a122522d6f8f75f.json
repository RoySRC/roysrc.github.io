[{"id":0,"href":"/docs/curriculum_vitae/","title":"Curriculum Vitae","section":"Docs","content":" Curriculum Vitae # Research Interests # I am interested in designing distributed, efficient, cache-friendly, and scalable data structures and algorithms that optimize the runtime performance of mathematical, scientific, and machine-learning applications. This includes mapping algorithms to parallel, new emergent, and state-of-the-art architectures for these domains. I am also interested in building compilers and developing compiler optimizations for all layers of the compiler stack to enhance performance further and reduce energy consumption.\nMy interest also lies in the application and development of machine learning techniques and algorithms for algorithmic and structured data problems such as network graphs, sequences, images, and time-series data. Of particular interest are applications of these ideas to solving problems in high-performance computing, bioinformatics/computational biology, data science, and computer vision.\nResearch Experience # University of Illinois Urbana Champaign (UIUC) # Research Software Engineer\nLed Quantitative Light Imaging Laboratory\u0026rsquo;s (QLI) efforts in software development for data acquisition and analysis, including software-hardware interfacing for microscopy systems and development and deployment of AI tools for solving biomedical problems under the supervision of the late Dr. Gabriel Popescu.\nWork closely with doctoral students from interdisciplinary research groups to develop new ML models for biomedical research with Spatial Light Interference Microscopy (SLIM) imaging modality.\nEnsured accurate and reliable data acquisition for extended imaging sessions of live and suspension cell cultures by leading the development efforts to solve the focus drift and tracking problem by developing and integrating new ML models into the C++ acquisition system using ONNX to provide a common format for ML model computation graphs, TensorRT for GPU kernel auto-tuning and computation graph optimization, CUDA, NVIDIA NPP for image processing in the inference pipeline, and cuDNN.\nAccelerated the application of AI to biomedical problems by designing an AutoML system as part of an internal tooling initiative to streamline the process of network architecture search, hyper-parameter tuning, and inference time optimization to be run on the NCSA (National Center for Supercomputing Application) compute clusters.\nReduced time and energy expended on manual data curation efforts by field experts by building an AI driven decision-making systems for intelligent image triggered acquisition by having the system eliminate frames with redundant or little to no information during the acquisition of live samples in bioreactor environments.\nColorado State University (CSU) # Performance Software Engineer\nWorked as a research assistant and performance software engineer under Dr. Anton Betten on optimizing the performance of Orbiter Computer Algebra system for classification of combinatorial objects.\nWrote object-oriented machine learning and BLAS templated library using CUDA C++. Standard BLAS packages could not be used as operations on group elements is not the same as operations on real numbers.\nUsed Deep Reinforcement Learning (DRL) with Graph Embeddings to optimize the Poset Classification algorithm by generating shallow Schreier trees.\nDeveloped object-oriented CUDA C++ code for accelerating rainbow clique backtrack searches on large graphs of more than 50,000 vertices\nQuadrupled the speed of polynomial long division used in Cyclic Redundancy Check by developing an equivalent parallel algorithm in CUDA C++.\nImplemented multithreaded CPU and CUDA versions of group theory algorithms to be run on Summit HPC Clusters\nOptimized graph theory algorithm implementation by an order of magnitude by making the underlying data structure more cache friendly.\nOpen-Source Software Contributions # Orbiter Computer Algebra System # GitHub Link Designed a custom context-free grammar system and expression parser with flex and bison for performing symbolic algebraic operations with Finite Field objects through successive graph rewrites based on rewrite rules.\nSolved two of the fundamental limitations of the visitor design pattern with 20% improvement in performance used in evaluation of expression trees over Finite Fields by using C++ template programming and automatic return type deduction.\nBuilt a parallel graph execution engine to automatically divide and schedule a Finite Field computation graph on parallel processors\nPublications # Autonomous Software-Driven Microscopy Focusing System # Authors: Sajeeb Roy Chowdhury, Masayoshi Sakakura, Neha Goswami, Young Jae Lee, Gabriel Popescu (Deceased)\nInvited talk at Stanford University (2022)\nPresented my work on building an accurate and reliable data acquisition system for extended imaging sessions of live and suspension cell cultures by developing an interpretable AI-driven solution to addressed the focus drift and tracking problem in bioreactor environments. Intelligent Image Triggered Acquisition # Authors: Sajeeb Roy Chowdhury, Gabriel Popescu (Deceased)\nSociety of Photo-Optical Instrumentation Engineers, SPIE 2022\nPresented my work on building AI driven decision-making systems for intelligent image acquisition by having the AI system eliminate frames with redundant or little to no information during the acquisition of live samples under bioreactor environments; increasing reliability and reducing manual data curation efforts by field experts. Machine Learning for Better Combinatorial Algorithms # Authors: Sajeeb Roy Chowdhury, Anton Betten\n50th Southeastern International Conference on Combinatorics, Graph Theory \u0026amp; Computing (2019) Presented my research on using Deep Reinforcement Learning (DRL) with Graph Embedding for combinatorial algorithm optimization by finding better algorithm heuristics for generating shallow schreier trees. A Rainbow Clique Search Algorithm for BLT-Sets # Authors: Abdullah Al-Azemi, Anton Betten, Sajeeb Roy Chowdhury\nInternational Conference on Mathematical Software (2018)\nPresented my research on using dynamic-parallelism on GPU for performing backtrack searches using the rainbow clique search algorithm, including discussing our strategy to handle thread divergence following a brief overview of the SIMT architecture. [Paper]\nLanguages and Tools # NOTE: Top languages do not indicate my skill level or anything similar to that, it\u0026rsquo;s a github metric of which languages I have the most code in on github. The statistics presented here also discount code committed to my self-hosted Gitea repository.\n"},{"id":1,"href":"/docs/publications/","title":"Publications","section":"Docs","content":" Autonomous Software-Driven Microscopy Focusing System # Authors: Sajeeb Roy Chowdhury, Masayoshi Sakakura, Neha Goswami, Young Jae Lee, Gabriel Popescu (Deceased)\nInvited talk at Stanford University (2022)\nPresented my work on building an accurate and reliable data acquisition system for extended imaging sessions of live and suspension cell cultures by developing an interpretable AI-driven solution to addressed the focus drift and tracking problem in bioreactor environments. Intelligent Image Triggered Acquisition # Authors: Sajeeb Roy Chowdhury, Gabriel Popescu (Deceased)\nSociety of Photo-Optical Instrumentation Engineers, SPIE 2022\nPresented my work on building AI driven decision-making systems for intelligent image acquisition by having the AI system eliminate frames with redundant or little to no information during the acquisition of live samples under bioreactor environments; increasing reliability and reducing manual data curation efforts by field experts. Machine Learning for Better Combinatorial Algorithms # Authors: Sajeeb Roy Chowdhury, Anton Betten\n50th Southeastern International Conference on Combinatorics, Graph Theory \u0026amp; Computing (2019) Presented my research on using Deep Reinforcement Learning (DRL) with Graph Embedding for combinatorial algorithm optimization by finding better algorithm heuristics for generating shallow schreier trees. A Rainbow Clique Search Algorithm for BLT-Sets # Authors: Abdullah Al-Azemi, Anton Betten, Sajeeb Roy Chowdhury\nInternational Conference on Mathematical Software (2018)\nPresented my research on using dynamic-parallelism on GPU for performing backtrack searches using the rainbow clique search algorithm, including discussing our strategy to handle thread divergence following a brief overview of the SIMT architecture. [Paper]\n"},{"id":2,"href":"/docs/publications/autonomous-software-driven-microscopy-focusing-system/","title":"Autonomous Software Driven Microscopy Focusing System","section":"Publications","content":" Autonomous Software-Driven Microscopy Focusing System # Abstract # Focus drift presents a substantial challenge in time-lapse microscopy imaging, especially during extended imaging sessions of live and suspension cell cultures in bioreactor environments. Existing methods, such as hardware-based adjustments or software solutions that rely on lasers and average phase value computations, often fall short by either being limited to specific types of cell cultures or by focusing on the debris rather than the sample. Recent advances include AI-driven solutions that directly adjust the focus based on the current observation in an end-to-end fashion. These systems work exceptionally well compared to pure hardware or software-based solutions. However, they lack safety measures and risk damage to the samples and to the microscope through objective-sample collision.\nTo address these challenges, we develop the Autonomous Software-Driven Microscopy Focusing System. This system introduces a machine learning based solution that significantly enhances focus stability and accuracy. Our novel approach utilizes an image-to-image translation task, where the model predicts in-focus images from the corresponding out-of-focus images. This predictive capability is then used to actively adjust the microscope\u0026rsquo;s focus, ensuring the observation closely matches the reference image, thereby mimicking the natural focusing method used by humans.\nFurthermore, to ensure inherent safety associated with our system, we pass the model inference through a post-inference pipeline algorithm that is responsible for making adjustments to the focus drive of the microscope.\nOur system is fine-tuned to handle dynamic changes in the imaging field without compromising sample safety. This method not only prevents focus drift but also supports the acquisition of high-quality images critical for reliable computational analysis. Demonstrated in real-time application in the CellVista acquisition software, this approach proves to be a robust solution, combining safety and interpretability, thus marking a significant advancement in biomedical imaging techniques.\nPresentation # [PDF Link]\n\u0026lsaquo; \u0026rsaquo; Demo # "},{"id":3,"href":"/docs/publications/iita/","title":"Iita","section":"Publications","content":" Intelligent Image Triggered Acquisition # Abstract # In bioreactor environments, the acquisition of live sample data often results in the collection of numerous frames with redundant or minimal information, leading to excessive time and energy spent on manual data curation by field experts. Current research in intelligent microscopy has primarily focused on improving the accuracy and speed of image analysis but has paid less attention to the efficiency and reliability of data acquisition itself. To address this gap, we develop the Intelligent Image Trigger Acquisition (IITA) system. This system employs an AI-driven decision-making framework to streamline the acquisition process. By incorporating an Efficient U-Net B5 architecture pretrained on ImageNet, the IITA system evaluates each frame in real-time to determine its relevance based on specific biological criteria such as cell count and viability. This approach significantly reduces the volume of non-informative data collected, thereby minimizing storage and computational demands. The system\u0026rsquo;s ability to selectively capture and store only pertinent frames reduces the need for extensive manual data curation, optimizing both time and energy resources. Results from live cell cultures in bioreactor settings demonstrate the systemâ€™s effectiveness in enhancing data quality and analysis efficiency. Future work will focus on expanding the system\u0026rsquo;s applicability to a wider range of cell types and biological conditions, further reducing the burden of manual intervention and paving the way for more autonomous biological research environments.\nPresentation # [PDF Link]\n\u0026lsaquo; \u0026rsaquo; "},{"id":4,"href":"/docs/publications/machine-learning-for-better-combinatorial-algorithms/","title":"Machine Learning for Better Combinatorial Algorithms","section":"Publications","content":" Machine Learning for Better Combinatorial Algorithms # Abstract # Classification of combinatorial objects leads to NP-hard problems in Computer Science. One such problem is computing the orbits of groups actions. These orbits can be represented using Schreier trees. These trees allow us to navigate the group using only the generating set. Good Schreier trees reduce the time taken to find the group element that maps one element of a coset to another by reducing the length of the action composition chain. These \u0026ldquo;good\u0026rdquo; trees are ideally shallow. An ideal tree is one where the depth is no more than 1. In an ideal tree there is no computation performed to find the mapping group element. Generating an ideal Schreier tree is only possible if the size of the generating set is approximately equal to the size of the group. This is however not possible for large groups.\nThus we have algorithms such as the Seress shallow Schreier tree algorithm for generating shallow trees by finding better generators. However, this often comes at the cost of increased generating set size. In this talk we present another way of generating shallow Schreier trees using machine learning techniques such as deep reinforcement learning and graph embedding. Emperical results show that our deep learning based algorithm performs as well as the Seress algorithm in terms of the depth of the tree while keeping the size of the generating set constant.\nPresentation # [PDF Link]\n\u0026lsaquo; \u0026rsaquo; "}]